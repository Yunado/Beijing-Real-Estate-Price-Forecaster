---
title: "Beijing Real Estate Price Prediction using Statistical Modeling"
author: "STAT 420, Summer 2022, B. Settles, Y. Ouyang, Y. Zheng"
date: '2022/07/31'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

# Introduction
## Beijing Real Estate Price Prediction using Statistical Modeling

## Description of the data file
The data file includes the Housing price of Beijing from 2011 to 2017, fetching from Lianjia.com (similar to Zillow or Redfin). It includes URL, ID, Lng, Lat, CommunityID, TradeTime, DOM (days on market), Followers, Total price, Price, Square, Living Room, Number of Drawing room, Kitchen and Bathroom, Building Type, Construction time, Renovation Condition, Building Structure, Ladder ratio (which is the proportion between number of residents on the same floor and number of elevator. It describes how many elevators a resident have on average), Elevator, Property Rights For Five Fears (It's related to China restricted purchase of houses policy), Subway, District, Community Average Price. Most data is collected from year 2011 - 2017, some of it is from Jan, 2018, and some is from earlier(2010, 2009).
All the data was fetching from https://bj.lianjia.com/chengjiao.

## Background information and source File
Background information: 

After some quick cleaning to remove the invalid values, we are left with 159376 obs. of  26 variables, from which we will select around 10 variables to build our model and test the performance with two splitted data sets - train and test, each might contain 80000 observations (depends on the calculation resources needed, we might reduce the number of observations used in building and testing the model).

Source file link: [Housing price in Beijing](https://www.kaggle.com/datasets/ruiqurm/lianjia)

## Statement of interest
Real estate price prediction is attractive for both holders and traders. It is an interesting topic since many factors can inflate the house price in Beijing. For example, we want to investigate how housing prices in Beijing are related to the growth of its economy. We will construct several statistical models to predict the data on Beijing's house prices. Specifically, we will utilize multiple linear regression, categorical predictors, transformations and model building using AIC, and BIC. We then will use model selection tools and model diagnostic methods to decide which model is the best model for predicting house prices. Finally, we will do a deep analysis of the best model to see its performance.

## Goal of the project
The final goal of the project is to find a model that best predicts the housing prices in Beijing for the future with the balance of error rate, complicity, and comprehensibility.

# Methods
## 1. Setup
### Import libraries 
```{r message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(faraway)
library(lmtest)
library(nortest)
```

### Set seed
```{r}
set.seed(120)
```

### Read file into R
```{r warning=FALSE}
housing = read_csv("Housing_price_in_Beijing.csv")
housing = as.data.frame(housing)
str(housing)
head(housing, 5)
```

### Clean up N/A and unused variables
```{r, warning = FALSE}
# Remove N/A values
sum(is.na(housing))
# Change Construction Time to numeric first to omit not finding N/A
housing$constructionTime = as.numeric(housing$constructionTime)
# Strip floor to remove characters
housing$floor = as.numeric(sapply(housing$floor, function(x) strsplit(x,' ')[[1]][2]))

housing = na.omit(housing)
sum(is.na(housing))

# Change TradeTime to numeric format
housing$tradeTime = as.numeric(as.POSIXct(housing$tradeTime, format = "%Y-%m-%d"))

# Remove irrelevant cols
# Since our goal is to predict the future house pricing,
# we are not interested in the technical side related to the website structure
# so remove the url as well as the id and followers of the listing
remove_cols = c("url", "id", "followers")
housing_cols_removed = housing[, -which(names(housing) %in% remove_cols)]
str(housing_cols_removed)
```
### Remove variables based on correlation
```{r}
# only consider the numeric values for correlation analysis
housing_cols_removed_numeric = housing_cols_removed[ , unlist(lapply(housing_cols_removed, is.numeric))]

hoursing_cor_res = cor(housing_cols_removed_numeric)

mean_housing_cor_res_beside_diag = mean(hoursing_cor_res[hoursing_cor_res != 1])

head(hoursing_cor_res, 5)
```
**The mean correlation result for the current housing dataset is `r mean_housing_cor_res_beside_diag` and since the values are relatively low, we decide not to further modify the variables based on it.**

### Remove variables based on multicollinearity 
```{r}
# Build an additive model to conduct the variance inflation factors analysis
housing_add_full = lm(totalPrice ~ ., housing_cols_removed_numeric)
vif(housing_add_full)
max_housing_add_full_vif = max(vif(housing_add_full))
```

**We only have one var - price that has a VIF sightly over 5 as `r max_housing_add_full_vif`, but as it's not significant, so we decide not to remove it from the list of variables.**

### Description of currently available variables
-**`Lng`: and `Lat` coordinates, using the BD09 protocol.**

-**`Cid`: the community id.**

-**`tradeTime`: the time of transaction.**

-**`DOM`: the active days on market.**

-**`totalPrice`: the total price of the listing**

-**`price`: the average price by square.**

-**`square`: the square of the house.**

-**`livingRoom`: the number of bedroom (updated based on the comment of the source).**

-**`drawingRoom`: the number of living room (updated based on the comment of the source).**

-**`kitchen`: the number of kitchen.**

-**`bathRoom`: the number of bathroom.**

-**`floor`: the height of the house, in number of floors.**

-**`buildingType`: including tower(1), bungalow(2)ï¼Œcombination of plate and tower(3), plate(4).**

-**`constructionTime`: the time of construction.**

-**`renovationCondition`: including other(1), rough(2), Simplicity(3), hardcover(4).**

-**`buildingStructure`: including unknow(1), mixed(2), brick and wood(3), brick and concrete(4), steel(5), and steel-concrete composite (6).**

-**`ladderRatio`: the proportion between number of residents on the same floor and number of elevator. It describes on average how many households are sharing the elevator. For example, the value would be 1 if there is only 1 household on the floor is using the elevator. And the value would be 6 if on the same floor, 6 different households/apartments are sharing the same elevator.**

-**`elevator`: have (1) or not have elevator(0).**

-**`fiveYearsProperty`: if the owner have the property for less than 5 years, similar to the 2 years rule in the US. If the the property is not owned for at least 5 years, the transaction will involve a higher tax payment.**

### Factor variables
Note that before model building, we will turn all categorical variables into factor variables, it will make more sense to work with factor variables for several categorical variables.
```{r}
housing_cols_removed$buildingType = as.factor(housing_cols_removed$buildingType)
housing_cols_removed$buildingStructure = as.factor(housing_cols_removed$buildingStructure)
housing_cols_removed$renovationCondition = as.factor(housing_cols_removed$renovationCondition)
housing_cols_removed$elevator = as.factor(housing_cols_removed$elevator)
housing_cols_removed$subway = as.factor(housing_cols_removed$subway)
housing_cols_removed$fiveYearsProperty = as.factor(housing_cols_removed$fiveYearsProperty)
housing_cols_removed$district = as.factor(housing_cols_removed$district)
```


## 2. Split data
Note that we split the data into two sets, the smaller data set has 2000 observations and the full data set has around 160,000 observations. This is because we want to consider the run time of model building and model selection. Since we subset our data randomly, there will be no bias so when we choose the final model with the smaller data set, we can then do the full comprehensive analysis of the full data set.

### Split dataset - large/full and small for quick analyses runtime
```{r}
# Get a subset of 2000 obs since the full dataset has ~ 160,000 obs
# for running analyses quickly
housing_2000 = sample_n(housing_cols_removed, 2000)
nrow(housing_2000)

# Remove the 2000 sampled obs from the full dataset
housing_full = housing_cols_removed %>% anti_join(housing_2000)
nrow(housing_full)

all.equal((nrow(housing_2000) + nrow(housing_full)), nrow(housing))
```

### Split dataset - train, validation, and test
#### Note that this is not used in our evlauation because we figured that train-test split isn't nessasary here. Spliting the data into small and large data set is enough to evaluate model performance.
We will utilize the train-test split when evaluating our model. Furthermore, we introduce the validation data set for mid-way validation on multiple model candidates for the best model. So we split the small and large data sets with 60% of training data, 20% of validation data and 20% of test data. The training data is used to train the model, then we use the validation data to find the best model. Last, we will use the test data to evaluate our best model.
```{r, eval = FALSE}
# The ratio we picked for train, validation, and test is 60-20-20
# First we sample the data for the small dataset
housing_2000_sample_size = nrow(housing_2000)
set_proportions = c(Training = 0.6, Validation = 0.2, Test = 0.2)
set_frequencies = diff(floor(housing_2000_sample_size * cumsum(c(0, set_proportions))))
housing_2000$set = sample(rep(names(set_proportions), times = set_frequencies))

housing_2000_tr = housing_2000[housing_2000$set == "Training", ]
housing_2000_va = housing_2000[housing_2000$set == "Validation", ]
housing_2000_te = housing_2000[housing_2000$set == "Test", ]

# Remove unused `set` var after dataset split
housing_2000_tr = subset(housing_2000_tr, select=-c(set))
housing_2000_va = subset(housing_2000_va, select=-c(set))
housing_2000_te = subset(housing_2000_te, select=-c(set))
```

```{r, eval = FALSE}
# Then we sample the data for the large dataset
housing_full_sample_size = nrow(housing_full)
set_proportions = c(Training = 0.6, Validation = 0.2, Test = 0.2)
set_frequencies = diff(floor(housing_full_sample_size * cumsum(c(0, set_proportions))))
housing_full$set = sample(rep(names(set_proportions), times = set_frequencies))

housing_full_tr = housing_full[housing_full$set == "Training", ]
housing_full_va = housing_full[housing_full$set == "Validation", ]
housing_full_te = housing_full[housing_full$set == "Test", ]

# Remove unused `set` var after dataset split
housing_full_tr = subset(housing_full_tr, select=-c(set))
housing_full_va = subset(housing_full_va, select=-c(set))
housing_full_te = subset(housing_full_te, select=-c(set))
```

## 3. Model analyses
### Helpers functions for model analysis
```{r}
# Shapiro-Wilk Test
get_sw_decision = function(model, alpha = 0.05){
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# Large sample Normality Test, Anderson-Darling Test
get_ad_decision = function(model, alpha = 0.05){
  decide = unname(ad.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# Breusch-Pagan Test
get_bp_decision = function(model, alpha = 0.05){
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

# Adjusted R-squared
get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

# Number of parameters
get_num_params = function(model) {
  length(coef(model))
}

# Variance Inflation Factor
get_big_vif = function(model) {
  sum(vif(model) > 5)
}

# Cross-Validated RMSE
get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

# Several analysis with Fitted vs Residual plot and Q-Q plot 
analysis = function(model) {
        par(mfrow = c(1, 3))
        plot(fitted(model),resid(model),pch = 16,xlab = "Fitted",ylab = "Residuals",main = "Fitted vs Residuals",col = "dodgerblue")
        abline(h = 0, lwd = 2, col = "darkorange")
        qqnorm(resid(model),pch = 16,main = "QQNorm",col = "darkgrey")
        qqline(resid(model),lwd = 2,col = "dodgerblue")
        hist(resid(model),main = "Residuals Histogram",col = "dodgerblue",border = "darkorange",xlab = "Residuals",ylab = "Frequency")
        mod_sw_test = shapiro.test(resid(model))
        mod_bp_test = bptest(model)

        knitr::kable(t(data.frame(
          "Shapiro-Wilk Test" =
            c("Test Statistic" = round(mod_sw_test$statistic, 5),
              "P-Value" = mod_sw_test$p.value,
              "Result" = ifelse(mod_sw_test$p.value < .05, "Reject", "Fail to Reject")),
          "Breusch-Pagan Test" =
            c("Test Statistic" = round(mod_bp_test$statistic, 5),
              "P-Value" = mod_bp_test$p.value,
              "Result" = ifelse(mod_bp_test$p.value < .05, "Reject", "Fail To Reject")))), col.names = c("Test Statistic", "P-Value", "Decision"))
}
```

### Histograms for variables
```{r, warning = FALSE}
par(mfrow = c(1, 3))
hist(housing_2000$Lng, xlab = "Lng", breaks = 20, main = "Lng", border="darkorange", col="dodgerblue")
hist(housing_2000$Lat, xlab = "Lat", breaks = 20, main = "Lat", border="darkorange", col="dodgerblue")
hist(housing_2000$tradeTime, xlab = "tradeTime", breaks = 20, main = "Trade Time", border="darkorange", col="dodgerblue")

par(mfrow = c(1, 3))
hist(housing_2000$totalPrice, xlab = "totalPrice", breaks = 20, main = "totalPrice", border="darkorange", col="dodgerblue")
hist(housing_2000$price, xlab = "price", breaks = 20, main = "price / s ft", border="darkorange", col="dodgerblue")
hist(housing_2000$square, xlab = "square", breaks = 20, main = "square feet", border="darkorange", col="dodgerblue")

par(mfrow = c(1, 3))
hist(housing_2000$livingRoom, xlab = "bedrooms", breaks = 20, main = "bedrooms", border="darkorange", col="dodgerblue")
hist(housing_2000$drawingRoom, xlab = "livingrooms", breaks = 20, main = "livingrooms", border="darkorange", col="dodgerblue")
hist(housing_2000$kitchen, xlab = "kitchen", breaks = 20, main = "kitchens", border="darkorange", col="dodgerblue")

par(mfrow = c(1, 3))
hist(housing_2000$bathRoom, xlab = "bathRoom", breaks = 20, main = "bathRooms", border="darkorange", col="dodgerblue")
hist(housing_2000$floor, xlab = "floor", breaks = 20, main = "floors", border="darkorange", col="dodgerblue")
hist(housing_2000$constructionTime, xlab = "constructionTime", breaks = 20, main = "constructionTime", border="darkorange", col="dodgerblue")

par(mfrow = c(1, 3))
hist(housing_2000$ladderRatio, xlab = "ladderRatio", breaks = 20, main = "ladderRatio", border="darkorange", col="dodgerblue")
hist(housing_2000$communityAverage, xlab = "communityAverage", breaks = 20, main = "communityAvg", border="darkorange", col="dodgerblue")
```

We first use histograms to check the frequency of each variable, to see if we need to future exclude variables that have very unbalanced data. As shown above, no variable need to be removed since all of them holds some useful information for our data set.

### Build model - additive, interative
We will build the additive model and the two-way interaction model first. Then we will check if the interactions are significant. From there we will utilize methods such as Transformation, Stepwise model selection and outlier diagnostics to further enhance our model performance.
```{r}
model_add = lm(totalPrice ~ ., data = housing_2000)
model_int = lm(totalPrice ~ . ^ 2, data = housing_2000)
```
We then will perform several analyses on the additive and the iterative model. We will check the four assumptions: Linearity, Independence, Normality and Equal Variance. Moreover, we will use ANOVA F-Test to see if the interaction term is significant. Then we will develop several other models with the base model as one of the additive model or the interactive model.
```{r}
analysis(model_add)
```
As we can see above, the additive model's Fitted vs Residuals graph looks off, though the spread of the variance is roughly the same at every fitted value with some outliers, the linearity assumption is violated. We see that the mean of the residuals is not always 0, the parabola shape of the Fitted vs Residuals graph suggested us higher order terms will help to overcome the linearity assumption.

From the Q-Q plot, the fat tails on both sides suggest that the normality assumption is violated, and the errors did not follow a normal distribution. Note that the histogram of the residuals shows a shape of a t-distribution instead of a normal distribution.

The Shapiro-Wilk test and the Breusch-Pagan test suggested that the constant variance and the normality assumption are indeed violated.

We will next analyze the interactive model. Then we will decide on the base model and try several improvements to fix the violation of assumptions.

```{r}
analysis(model_int)
```
As we can see above, the Fitted vs Residuals graph for the interactive model looks much better when compared to the additive model. The linearity assumption holds. But the normality assumption is violated from the Q-Q plot. Moreover from the formal testing, we see that the constant variance assumption holds but the normality assumption is violated. We will now test the significance of the interaction terms compared to the additive model, to see which model will be used as the base model.
```{r}
anova(model_add, model_int)
anova(model_add, model_int)[2, "Pr(>F)"]
```
Based on the ANOVA F-test, we obtain a p-value of `r anova(model_add, model_int)[2, "Pr(>F)"]` which is very small, meaning that we reject the null hypothesis, we prefer the interactive model as the base model instead of the additive model.

### Transformations
Since we will use the interactive model as our base model, we can try to apply some transformations to see if it helps our model performance.

## log transformations
```{r}
model_log_price = lm(log(totalPrice) ~ . ^ 2, data = housing_2000)
analysis(model_log_price)
```
Let us apply the log transformation to the response variable TotalPrice, we see that the logging on the TotalPrice did make the Fitted vs Residuals graph more centred at 0 but it introduced several influential points. Similarly, it makes the Q-Q plot tails less prominent but added some outliers. Let us filter out the influential points to see if there are any changes.

```{r}
cd_log_price = cooks.distance(model_log_price)
model_log_price_fix = lm(log(totalPrice) ~ . ^ 2, data = housing_2000, 
                         subset = cd_log_price < 4 / length(cd_log_price))
analysis(model_log_price_fix)
```
Note that we can see that the linearity assumption holds now, since we modify the trading time in our data from data time format to seconds, maybe logging on the trade time will help. Let us try logging the predictors and polynomial transform the predictors to see if there will be any improvement.

## log and polynomial transformation
```{r}
model_log_poly_tradetime = lm(log(totalPrice) ~ . + log(tradeTime) + 
                                I(square ^ 2) + I(ladderRatio ^ 2) + 
                                I(bathRoom ^ 2), data = housing_2000)
analysis(model_log_poly_tradetime)
cd_log_poly_tradetime = cooks.distance(model_log_poly_tradetime)
model_log_poly_fix_tradetime = lm(log(totalPrice) ~ . + log(tradeTime) + 
                                    I(square ^ 2) + I(ladderRatio ^ 2) +
                                    I(bathRoom^2), data = housing_2000, 
                                  subset = cd_log_poly_tradetime < 
                                    4 / length(cd_log_poly_tradetime))
analysis(model_log_poly_fix_tradetime)
vif(model_log_poly_fix_tradetime)[vif(model_log_poly_fix_tradetime) > 5]
```

The log of trade time turned out to have a very large VIF value so we would prefer not to use it.

```{r}
model_log_poly_dom = lm(log(totalPrice) ~ . + log(DOM) + I(square ^ 2) +
                          I(bathRoom ^ 2), data = housing_2000)
analysis(model_log_poly_dom)

cd_log_poly_dom = cooks.distance(model_log_poly_dom)
model_log_poly_fix_dom = lm(log(totalPrice) ~ . + log(DOM) + I(square ^ 2) +
                              I(bathRoom ^ 2), data = housing_2000,
                            subset = cd_log_poly_dom < 
                              4 / length(cd_log_poly_dom))
analysis(model_log_poly_fix_dom)
```
We applied several polynomials and log transforms onto the predictors. We see that the linearity assumption holds in both cases. Remove the influential points made the p-value of the Shapiro-Wilk test slightly larger. We will now rely on AIC and BIC step-wise variable selection to see if there will be any improvement (planned and attempted, but due to limited computation power we weren't able to generate useful models, please check Appendix for our implementation).

# Results
## Result Table
```{r warning=FALSE}
knitr::kable(data.frame(
  sw_decision = c(get_sw_decision(model_int),
                  get_sw_decision(model_log_price),
                  get_sw_decision(model_log_price_fix),
                  get_sw_decision(model_log_poly_tradetime),
                  get_sw_decision(model_log_poly_fix_tradetime),
                  get_sw_decision(model_log_poly_dom),
                  get_sw_decision(model_log_poly_fix_dom)),
  
  bp_decision = c(get_bp_decision(model_int),
                  get_bp_decision(model_log_price),
                  get_bp_decision(model_log_price_fix),
                  get_bp_decision(model_log_poly_tradetime),
                  get_bp_decision(model_log_poly_fix_tradetime),
                  get_bp_decision(model_log_poly_dom),
                  get_bp_decision(model_log_poly_fix_dom)),
  
  loocv_rmse = c(get_loocv_rmse(model_int),
                 get_loocv_rmse(model_log_price),
                 get_loocv_rmse(model_log_price_fix),
                 get_loocv_rmse(model_log_poly_tradetime),
                 get_loocv_rmse(model_log_poly_fix_tradetime),
                 get_loocv_rmse(model_log_poly_dom),
                 get_loocv_rmse(model_log_poly_fix_dom)),
  
  adj_r2 = c(get_adj_r2(model_int),
             get_adj_r2(model_log_price),
             get_adj_r2(model_log_price_fix),
             get_adj_r2(model_log_poly_tradetime),
             get_adj_r2(model_log_poly_fix_tradetime),
             get_adj_r2(model_log_poly_dom),
             get_adj_r2(model_log_poly_fix_dom)),
  
  num_params = c(get_num_params(model_int),
                 get_num_params(model_log_price),
                 get_num_params(model_log_price_fix),
                 get_num_params(model_log_poly_tradetime),
                 get_num_params(model_log_poly_fix_tradetime),
                 get_num_params(model_log_poly_dom),
                 get_num_params(model_log_poly_fix_dom)),
  
  big_vif = c(get_big_vif(model_int),
              get_big_vif(model_log_price),
              get_big_vif(model_log_price_fix),
              get_big_vif(model_log_poly_tradetime),
              get_big_vif(model_log_poly_fix_tradetime),
              get_big_vif(model_log_poly_dom),
              get_big_vif(model_log_poly_fix_dom)),
  
  row.names = c("model_int",
                "model_log_price",
                "model_log_price_fix",
                "model_log_poly_tradetime",
                "model_log_poly_fix_tradetime",
                "model_log_poly_dom",
                "model_log_poly_fix_dom")))
```


```{r}
vif(model_log_poly_fix_dom)[vif(model_log_poly_fix_dom) > 5]
```

We observe the overall improvements (increase of `adj_r2` and decrease of `loocv_rmse`, `num_params` and `big_vif`) as we fix the `log` models, and yield the result of a winner `model_log_poly_fix_dom` that has relatively lower `loocv_rmse`, higher `adj_r2` and significantly lower `num_params` and `big_vif`. However. Though this model rejects the null hypothesis in both tests, we also observe a similar behavior in other models, we believe that this is due to our data contains many categorical variables. We see that the interaction model passed the constant variance assumption with 669 parameters. That is simply too much for a linear model, at the end, `model_log_poly_fix_dom` is still the best model consider the interpretability, the predicting performance and the LOOCV RMSE with the adjusted R square.

Also, in the model, we have 11 significant (> 5) VIF values, which could use further improvements. However, most of them are not significantly larger than 5, and 3 out of 5 that are much larger are polynomial values.

Therefore, this model still seems to perform better than the rest of the candidates. Note that we can improve the model futhur with with AIC and BIC. (more details in Appendix).

## Result formula
```{r}
formula(model_log_poly_fix_dom)
```

Now since we obtain the best model, we will evaluate it once more with the whole dataset.
```{r}
model_log_poly_dom_L = lm(log(totalPrice) ~ . + log(DOM) + I(square ^ 2) +
                          I(bathRoom ^ 2), data = housing_full)

cd_log_poly_dom_L = cooks.distance(model_log_poly_dom_L)
model_log_poly_fix_dom_L = lm(log(totalPrice) ~ . + log(DOM) + I(square ^ 2) +
                              I(bathRoom ^ 2), data = housing_full,
                            subset = cd_log_poly_dom_L < 
                              4 / length(cd_log_poly_dom_L))

knitr::kable(data.frame(
  ad_decision = get_ad_decision(model_log_poly_fix_dom_L),
  bp_decision = get_bp_decision(model_log_poly_fix_dom_L),
  loocv_rmse = get_loocv_rmse(model_log_poly_fix_dom_L),
  adj_r2 = get_adj_r2(model_log_poly_fix_dom_L),
  num_params = get_num_params(model_log_poly_fix_dom_L),
  big_vif = get_big_vif(model_log_poly_fix_dom_L),
  row.names = "model_log_poly_fix_dom_L"))
```

Our housing price model, overall, seems to be valid, with the model having a relatively high adjusted $R ^ 2$ and a low LOOCV RMSE. We detected a relationship between the log of the totalPrice variable and the other predictors, which gives insight into what factors can influence the housing price. With the help of this model, we believe that we could estimate a price for a house in Beijing after knowing the information about its square footage, longitude, latitude, living rooms, bathrooms, and kitchens, among other variables.
 
Although the model that we created did not pass the normality and constant variance assumptions, it passed the linearity assumption, meaning that our model is still useful for predicting housing prices. The violation of the other two assumptions is mainly due to the dataset we have, we believe that the housing market in Beijing is violative and very uncertain. We decide to choose the smaller model as the final model because the complex and larger model may be overfitting and end up not being that accurate at predicting housing prices. 
 
It should be kept in mind that while the predictors in our model are objective and quantifiable, the pricing process is subjective and can be influenced by many factors. Some other factors that could affect pricing are the quality of the furniture or appliances, what's nearby of the house, the economic situation in the neighbourhood, and many more factors. 

We believe that the model created could be a valuable reference when estimating Beijing housing prices, which could be used by those interested in buying or selling a house in Beijing.


# Appendix

## Team Members
- Bryan Settles (brsettl2@illinois.edu)
- Yixing Zheng (yixingz3@illinois.edu)
- Yunfei Ouyang (yunfeio2@illinois.edu)

### Search functions - both direction with AIC and BIC
**We attempted to this analysis, but due to the resources required and our computation power, we were not able to calculate the result.** 
The base model considered here is the interaction model with the log transformation without influential points. The log interaction model is generally better than the log-poly transform model because it passed the BP test.
```{r}
base_model = model_log_price_fix
```

## Backward search with AIC and BIC
We will first try backward search with AIC and BIC.
```{r, eval = FALSE}
aic_back = step(base_model, direction = "backward", k = 2, trace = 0)
n = length(resid(base_model))
bic_back = step(base_model, direction = "backward", k = log(n), trace = 0)
```
We will also try to search in forward direction where the maximum scope is our base model.
```{r, eval = FALSE}
model_start = lm(log(totalPrice) ~ 1, data = housing_2000_tr)
aic_forward = step(model_start, direction = "forward", 
                   scope = log(totalPrice) ~ . ^ 2,
                   k = 2, trace = 0)
n = length(resid(base_model))
bic_forward = step(model_start, direction = "forward", 
                   scope = log(totalPrice) ~ . ^ 2,
                   k = log(n), trace = 0)
```
**Original goal: Compare the four models, find the best one as the best model. Yields the result.**

## Misc.

```{r, eval = FALSE}
area_mod = lm(square ~ livingRoom + drawingRoom + bathRoom + kitchen, data = housing_2000_tr)
summary(area_mod)$r.squared

log_mod = lm(log(price) ~ . + I(square ^ 2) + I(ladderRatio ^ 2) + I(buildingStructure ^ 2) + I(buildingType) + log(buildingStructure) + log(buildingType), data = housing_2000_tr)

big_mod = lm(price ~ . + I(square ^ 2) + I(ladderRatio ^ 2) + I(buildingStructure ^ 2) + I(buildingType) + log(buildingStructure) + log(buildingType), data = housing_2000_tr)

model_add = lm(price ~ ., data = housing_2000_tr)
summary(model_add)$adj.r.squared

aic_add_mod = step(model_add, direction = "backward", trace = 0)
house_data_aic_add = housing_2000_tr[-which(cooks.distance(aic_add_mod) > 4 / length(cooks.distance(aic_add_mod))),]
aic_add_mod = lm(formula = aic_add_mod, data = house_data_aic_add)

bic_add_mod = step(model_add, direction = "backward", k = log(length(resid(model_add))), trace = 0)
house_data_bic_add = housing_2000_tr[-which(cooks.distance(bic_add_mod) > 4 / length(cooks.distance(bic_add_mod))),]
bic_add_mod = lm(formula = bic_add_mod, data = house_data_bic_add)

aic_big_mod = step(big_mod, direction = "backward", trace = 0)
house_data_aic_big = housing_2000_tr[-which(cooks.distance(aic_big_mod) > 4 / length(cooks.distance(aic_big_mod))),]
aic_big_mod = lm(formula = aic_big_mod, data = house_data_aic_big)

bic_big_mod = step(big_mod, direction = "backward", k = log(length(resid(model_add))), trace = 0)
house_data_bic_big = housing_2000_tr[-which(cooks.distance(bic_big_mod) > 4 / length(cooks.distance(bic_big_mod))),]
bic_big_mod = lm(formula = bic_big_mod, data = house_data_bic_big)

aic_log_mod = step(log_mod, direction = "backward", trace = 0)
house_data_aic_log = housing_2000_tr[-which(cooks.distance(aic_log_mod) > 4 / length(cooks.distance(aic_log_mod))),]
aic_log_mod = lm(formula = aic_log_mod, data = house_data_aic_log)

bic_log_mod = step(log_mod, direction = "backward", k = log(length(resid(model_add))), trace = 0)
house_data_bic_log = housing_2000_tr[-which(cooks.distance(bic_log_mod) > 4 / length(cooks.distance(bic_log_mod))),]
bic_log_mod = lm(formula = bic_log_mod, data = house_data_bic_log)



knitr::kable(data.frame(sw_decision = c(get_sw_decision(model_add),get_sw_decision(area_mod),get_sw_decision(big_mod),get_sw_decision(log_mod),get_sw_decision(aic_add_mod),
                                            get_sw_decision(bic_add_mod),
                                            get_sw_decision(aic_big_mod),
                                            get_sw_decision(bic_big_mod),
                                            get_sw_decision(aic_log_mod),
                                            get_sw_decision(bic_log_mod)),
                            bp_decision = c(get_bp_decision(model_add),get_bp_decision(area_mod),get_bp_decision(big_mod),get_bp_decision(log_mod),get_bp_decision(aic_add_mod),
                                            get_bp_decision(bic_add_mod),
                                            get_bp_decision(aic_big_mod),
                                            get_bp_decision(bic_big_mod),
                                            get_sw_decision(aic_log_mod),
                                            get_sw_decision(bic_log_mod)),
                            loocv_rmse = c(get_loocv_rmse(model_add),get_loocv_rmse(area_mod),get_loocv_rmse(big_mod),get_loocv_rmse(log_mod),get_loocv_rmse(aic_add_mod),
                                           get_loocv_rmse(bic_add_mod),
                                           get_loocv_rmse(aic_big_mod),
                                           get_loocv_rmse(bic_big_mod),
                                           get_loocv_rmse(aic_log_mod),
                                           get_loocv_rmse(bic_log_mod)),
                            adj_r2 = c(get_adj_r2(model_add),get_adj_r2(area_mod),get_adj_r2(big_mod),get_adj_r2(log_mod),get_adj_r2(aic_add_mod),
                                       get_adj_r2(bic_add_mod),
                                       get_adj_r2(aic_big_mod),
                                       get_adj_r2(bic_big_mod),
                                       get_adj_r2(aic_log_mod),
                                       get_adj_r2(bic_log_mod)),
                            num_params = c(get_num_params(model_add),get_num_params(area_mod),get_num_params(big_mod),get_num_params(log_mod),get_num_params(aic_add_mod),
                                           get_num_params(bic_add_mod),
                                           get_num_params(aic_big_mod),
                                           get_num_params(bic_big_mod),
                                           get_num_params(aic_log_mod),
                                           get_num_params(bic_log_mod)),
                            big_vif = c(get_big_vif(model_add),get_big_vif(area_mod),get_big_vif(big_mod),get_big_vif(log_mod),get_big_vif(aic_add_mod),
                                        get_big_vif(bic_add_mod),
                                        get_big_vif(aic_big_mod),
                                        get_big_vif(bic_big_mod),
                                        get_big_vif(aic_log_mod),
                                        get_big_vif(bic_log_mod)),
                            row.names = c("model_add","area_mod","big_mod","log_mod","aic_add_mod", 
                                          "bic_add_mod", 
                                          "aic_big_mod", 
                                          "bic_big_mod",
                                          "aic_log_mod",
                                          "bic_log_mod")))

analysis(model_add)
analysis(area_mod)
analysis(big_mod)
analysis(log_mod)
analysis(aic_add_mod)
analysis(bic_add_mod)
analysis(aic_big_mod)
analysis(bic_big_mod)
analysis(aic_log_mod)
analysis(bic_log_mod)

new_add_model =  lm(price ~ ., data = housing_2000_tr, subset = cooks.distance(model_add) <= (4 / nrow(housing_2000_tr)))
new_add_model_AIC = step(new_add_model, direction = "backward", trace = 0)

knitr::kable(data.frame(sw_decision = c(get_sw_decision(new_add_model),get_sw_decision(new_add_model_AIC)),
                            bp_decision = c(get_bp_decision(new_add_model),get_bp_decision(new_add_model_AIC)),
                            loocv_rmse = c(get_loocv_rmse(new_add_model),get_loocv_rmse(new_add_model_AIC)),
                            adj_r2 = c(get_adj_r2(new_add_model),get_adj_r2(new_add_model_AIC)),
                            num_params = c(get_num_params(new_add_model),get_num_params(new_add_model_AIC)),
                            big_vif = c(get_big_vif(new_add_model),get_big_vif(new_add_model_AIC)),
                            row.names = c("new_add_model","new_add_model_AIC")))

analysis(new_add_model)
analysis(new_add_model_AIC)
```

